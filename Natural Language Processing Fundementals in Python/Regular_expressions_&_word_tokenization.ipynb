{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regular expressions & word tokenization",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "xxUPzhEUQie6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11b042db-8c4f-434f-9fd8-45b07410b10d"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "my_string = \"Let's write RegEx!\"\n",
        "PATTERN = r\"\\w+\"\n",
        "re.findall(PATTERN, my_string)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', 's', 'write', 'RegEx']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "nNxvO0PwQ8gt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Practicing regular expressions: re.split() and re.findall()\n",
        "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
        "\n",
        "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line."
      ]
    },
    {
      "metadata": {
        "id": "VWC8JX6dQqyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c575822e-ffe6-46b6-9026-54bec15e928a"
      },
      "cell_type": "code",
      "source": [
        "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
        "# Import the regex module\n",
        "import re\n",
        "\n",
        "# Write a pattern to match sentence endings: sentence_endings\n",
        "sentence_endings = r\"[.?!]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "print(re.split(sentence_endings,my_string))\n",
        "# Find all capitalized words in my_string and print the result\n",
        "capitalized_words = r\"[A-Z]\\w+\"\n",
        "print(re.findall(capitalized_words,my_string))\n",
        "\n",
        "# Split my_string on spaces and print the result\n",
        "spaces = \"\\s+\"\n",
        "print(re.split(spaces,my_string))\n",
        "\n",
        "# Find all digits in my_string and print the result\n",
        "digits = \"\\d+\"\n",
        "print(re.findall(digits,my_string))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
            "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
            "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
            "['4', '19']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dVhQsrricX7M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Word tokenization with NLTK**\n",
        "\n",
        "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell!\n",
        "\n",
        "Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail."
      ]
    },
    {
      "metadata": {
        "id": "kUhFJgIxSW1n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "14cc8d4d-6fa9-46dd-cb23-70b3b79d4e30"
      },
      "cell_type": "code",
      "source": [
        "scene_one = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n",
        "# Import necessary modules\n",
        "# the below statements were added when a run time error was observed\n",
        "import nltk\n",
        "nltk.download('punkt') \n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split scene_one into sentences: sentences\n",
        "sentences = sent_tokenize(scene_one)\n",
        "\n",
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n",
        "\n",
        "# Print the unique tokens result\n",
        "print(unique_tokens)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "{'coconut', 'south', 'martin', 'its', 'question', 'Wait', 'velocity', 'winter', 'must', 'since', 'migrate', 'trusty', 'master', 'plover', 'No', \"'em\", 'in', 'zone', 'ridden', 'swallows', \"'\", 'Well', 'search', 'warmer', 'breadth', 'through', 'Am', 'air-speed', 'times', 'are', 'the', 'these', 'five', 'non-migratory', 'simple', 'Britons', 'servant', 'climes', 'sun', 'beat', 'European', 'court', 'other', 'Are', 'knights', '--', 'grip', 'anyway', 'with', 'bring', ':', 'castle', 'length', 'I', 'wants', 'tell', 'snows', 'carrying', 'land', 'does', 'seek', 'all', 'but', 'We', 'point', 'lord', 'grips', 'and', 'why', 'to', 'That', 'halves', 'there', 'ounce', 'Arthur', 'from', 'ratios', 'me', 'Mercea', 'minute', 'Patsy', 'together', 'or', \"'m\", 'guiding', 'The', 'could', 'an', 'bird', 'SOLDIER', 'creeper', 'matter', 'horse', 'order', 'maintain', 'wind', 'get', 'It', 'will', 'it', 'ask', 'where', 'held', 'tropical', 'every', 'them', 'ARTHUR', 'carried', 'What', 'husk', 'Not', 'wings', 'Pull', 'on', 'maybe', 'kingdom', 'by', 'Pendragon', 'course', 'pound', 'African', \"'s\", 'you', 'back', 'strand', '!', 'They', 'your', '...', 'Whoa', 'SCENE', \"'d\", 'not', 'covered', 'Please', 'using', 'Listen', 'Found', 'then', \"'re\", 'carry', 'is', '.', 'dorsal', '#', 'needs', 'our', 'So', 'right', 'at', 'Camelot', 'forty-three', 'yeah', 'line', 'feathers', 'of', 'But', 'coconuts', 'mean', '?', 'sovereign', 'my', 'Yes', 'be', 'Will', 'may', 'got', 'fly', 'defeator', 'Saxons', 'am', 'speak', \"n't\", 'interested', 'clop', 'house', 'weight', 'Supposing', '1', 'go', 'swallow', 'second', 'Where', 'they', 'In', 'yet', '2', 'if', \"'ve\", 'Uther', 'agree', 'use', 'You', 'suggesting', 'goes', 'bangin', 'A', 'do', 'empty', '[', 'King', 'just', 'that', 'England', 'Who', 'join', ']', 'Oh', 'here', 'temperate', 'son', 'two', 'Halt', 'this', 'one', 'found', 'strangers', 'KING', 'Ridden', 'under', 'Court', 'he', 'a', 'have', ',', 'who'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZBr0Xg_Yed-u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**More regex with re.search()**\n",
        "\n",
        "In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
        "\n",
        "You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text."
      ]
    },
    {
      "metadata": {
        "id": "HJ3TQM17eiho",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}